from typing import Callable, Dict, List, Optional
from pathlib import Path
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
from opensearchpy import OpenSearch, RequestsHttpConnection
from haystack.nodes.retriever import EmbeddingRetriever
from haystack.document_stores import OpenSearchDocumentStore
from haystack.utils import clean_wiki_text, fetch_archive_from_http, print_answers

import json
import sys
import os 

from haystack.nodes.file_converter import BaseConverter, DocxToTextConverter, PDFToTextConverter, TextConverter, MarkdownConverter
from haystack.schema import Document

import logging

logger = logging.getLogger(__name__)

# Add markdown conversion
# Licensed under Apache-2.0 license from deepset-ai haystack
# https://github.com/deepset-ai/haystack/blob/ba30971d8d77827da9d2c81d82f7d02bf1917d8c/haystack/utils/preprocessing.py
def convert_files_to_docs(
    dir_path: str,
    clean_func: Optional[Callable] = None,
    split_paragraphs: bool = False,
    encoding: Optional[str] = None,
    id_hash_keys: Optional[List[str]] = None,
) -> List[Document]:
    """
    Convert all files(.txt, .pdf, .docx) in the sub-directories of the given path to Documents that can be written to a
    Document Store.

    :param dir_path: The path of the directory containing the Files.
    :param clean_func: A custom cleaning function that gets applied to each Document (input: str, output: str).
    :param split_paragraphs: Whether to split text by paragraph.
    :param encoding: Character encoding to use when converting pdf documents.
    :param id_hash_keys: A list of Document attribute names from which the Document ID should be hashed from.
            Useful for generating unique IDs even if the Document contents are identical.
            To ensure you don't have duplicate Documents in your Document Store if texts are
            not unique, you can modify the metadata and pass [`"content"`, `"meta"`] to this field.
            If you do this, the Document ID will be generated by using the content and the defined metadata.
    """
    file_paths = [p for p in Path(dir_path).glob("**/*")]
    allowed_suffixes = [".pdf", ".txt", ".docx", ".md"]
    suffix2converter: Dict[str, BaseConverter] = {}

    suffix2paths: Dict[str, List[Path]] = {}
    for path in file_paths:
        file_suffix = path.suffix.lower()
        if file_suffix in allowed_suffixes:
            if file_suffix not in suffix2paths:
                suffix2paths[file_suffix] = []
            suffix2paths[file_suffix].append(path)
        elif not path.is_dir():
            logger.warning(
                "Skipped file {0} as type {1} is not supported here. "
                "See haystack.file_converter for support of more file types".format(path, file_suffix)
            )

    # No need to initialize converter if file type not present
    for file_suffix in suffix2paths.keys():
        if file_suffix == ".pdf":
            suffix2converter[file_suffix] = PDFToTextConverter()
        if file_suffix == ".txt":
            suffix2converter[file_suffix] = TextConverter()
        if file_suffix == ".docx":
            suffix2converter[file_suffix] = DocxToTextConverter()
        if file_suffix == ".md":
            suffix2converter[file_suffix] = MarkdownConverter()

    documents = []
    for suffix, paths in suffix2paths.items():
        for path in paths:
            logger.info("Converting {}".format(path))
            # PDFToTextConverter, TextConverter, and DocxToTextConverter return a list containing a single Document
            document = suffix2converter[suffix].convert(
                file_path=path, meta=None, encoding=encoding, id_hash_keys=id_hash_keys
            )[0]
            text = document.content

            if clean_func:
                text = clean_func(text)

            if split_paragraphs:
                for para in text.split("\n\n"):
                    if not para.strip():  # skip empty paragraphs
                        continue
                    documents.append(Document(content=para, meta={"name": path.name}, id_hash_keys=id_hash_keys))
            else:
                documents.append(Document(content=text, meta={"name": path.name}, id_hash_keys=id_hash_keys))

    return documents



host = os.environ['OPENSEARCH_HOST']
password = os.environ['OPENSEARCH_PASSWORD']

doc_dir_aws = "/awsdocs/data"
if len(sys.argv)>1:
  doc_dir_aws = sys.argv[1]

print(f"doc_dir_aws {doc_dir_aws}")

document_store = OpenSearchDocumentStore(
        host = host,
        port = 443,
        username = 'admin',
        password = password,
        scheme = 'https',
        verify_certs = False,
        similarity='cosine'
    )

dicts_aws = convert_files_to_docs(dir_path=doc_dir_aws, clean_func=clean_wiki_text, split_paragraphs=True)

path = Path(doc_dir_aws)

# Let's have a look at the first 3 entries:
print("First 3 documents to be ingested")
print(dicts_aws[:3])

print(f"Starting Ingestion, Documents: {len(dicts_aws)}")

# Now, let's write the dicts containing documents to our DB.
document_store.write_documents(dicts_aws, index="awsdocs")

print(f"Finished Ingestion, Documents: {len(dicts_aws)}")

print(f"Started Update Embeddings, Documents: {len(dicts_aws)}")
# Calculate and store a dense embedding for each document
retriever = EmbeddingRetriever(
    document_store=document_store,
    model_format = "sentence_transformers",
    embedding_model = "sentence-transformers/all-mpnet-base-v2"
)
document_store.update_embeddings(
    retriever=retriever,
    index="awsdocs"
)
print(f"Finished Update Embeddings, Documents: {len(dicts_aws)}")